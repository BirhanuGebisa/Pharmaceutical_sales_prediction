{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model with deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM \n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The load the raw sales data\n",
    "data_path = 'data/train_store_unlabel.csv'\n",
    "version = 'vsl2'\n",
    "repo = '../'\n",
    "\n",
    "train_df = File_handler.dvc_get_data(data_path, version, repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the Rossmann Store Sales dataset into time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.groupby(\"Date\").agg({\"Sales\": \"mean\"})\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether your time Series Data is Stationary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfResult = adfuller(data.Sales.values, autolag='AIC')\n",
    "print(f'ADF Statistic: {adfResult[0]}')\n",
    "print(f'p-value: {adfResult[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "try:\n",
    "    scaler.fit(data.Sales.values.reshape([-1, 1]))\n",
    "    scaled_array = scaler.transform(data.Sales.values.reshape(-1, 1))\n",
    "    data['SalesScaled'] = scaled_array\n",
    "    data.tail(10)\n",
    "\n",
    "    my_logger.debug(\"Data scaled successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    my_logger.exception(f\"Scaling error, {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Scaled Sales is Stationary\n",
    "\n",
    "adfResult = adfuller(data.SalesScaled.values, autolag='AIC')\n",
    "print(f'ADF Statistic: {adfResult[0]}')\n",
    "print(f'p-value: {adfResult[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "salesScaledDiff = difference(data.SalesScaled.values)\n",
    "salesScaledDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(data.SalesScaled)\n",
    "WINDOW_SIZE = 48\n",
    "BATCH_SIZE = SIZE - WINDOW_SIZE * 2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateTrain = data.index.values[0:BATCH_SIZE]\n",
    "DateValid = data.index.values[BATCH_SIZE:]\n",
    "XTrain = data.SalesScaled.values[0:BATCH_SIZE].astype('float32')\n",
    "XValid = data.SalesScaled.values[BATCH_SIZE:].astype('float32')\n",
    "\n",
    "# Obtain shapes for vectors of size (,1) for dates series\n",
    "\n",
    "DateTrain = np.reshape(DateTrain, (-1, 1))\n",
    "DateValid = np.reshape(DateValid, (-1, 1))\n",
    "\n",
    "print(\"Shape of the training set date series: \", DateTrain.shape)\n",
    "print(\"Shape of the validation set date series: \", DateValid.shape)\n",
    "print()\n",
    "print(\"Shape of the training set logarithm of sales series: \", XTrain.shape)\n",
    "print(\"Shape of the validation set logarithm of sales series in a stateless LSTM: \", XValid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "# add extra dimension\n",
    "series = tf.expand_dims(XTrain, axis=-1)\n",
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor from each individual element\n",
    "dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a window_size + 1 chunk from the slices\n",
    "dataset = dataset.window(WINDOW_SIZE + 1, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetEx = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "datasetEx = datasetEx.window(5, shift=1, drop_remainder=True)\n",
    "for window in datasetEx:\n",
    "    print([elem.numpy() for elem in window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(WINDOW_SIZE + 1))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE):\n",
    "  series = tf.expand_dims(series, axis=-1)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTrain = windowed_dataset(XTrain)\n",
    "DatasetVal = windowed_dataset(XValid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a LSTM Regression model to predict the next sale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(8, input_shape=[None, 1], return_sequences=True))\n",
    "model.add(LSTM(4, input_shape=[None, 1]))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"huber_loss\", optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(DatasetTrain, epochs=EPOCHS, validation_data=DatasetVal, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(DatasetVal, verbose=1)\n",
    "t_loss = model.evaluate(DatasetTrain, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training loss = %f\" % (t_loss))\n",
    "print(\"Validation loss = %f\" % (loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "time = strftime(\"%Y-%m-%d\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../models/LSTM_sales_prediction_model {time}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True) \n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(SIZE).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "\n",
    "Forecast = model_forecast(model, data.SalesScaled.values[:, np.newaxis], WINDOW_SIZE)\n",
    "Results = Forecast[BATCH_SIZE-WINDOW_SIZE:-1]\n",
    "Results1 = scaler.inverse_transform(Results.reshape(-1,1))\n",
    "XValid1 = scaler.inverse_transform(XValid.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 8))\n",
    "plt.title(\"LSTM Model Forecast Compared to Validation Data\")\n",
    "plt.plot(DateValid.astype('datetime64'), Results1, label='Forecast series')\n",
    "plt.plot(DateValid.astype('datetime64'), np.reshape(XValid1, (2*WINDOW_SIZE, 1)), label='Validation series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Thousands of Units')\n",
    "plt.xticks(DateValid.astype('datetime64')[:,-1], rotation = 90) \n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c1d84affd04ea94635ae004ab92d8f7c59753998561fcf20a79a729cfcfa1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
